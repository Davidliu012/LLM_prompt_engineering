{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from bigbench.api import json_task\n",
    "import bigbench.api.model as api_model\n",
    "import bigbench.models.model_utils as model_utils\n",
    "from bigbench.api.task import ScoreData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONAL_STIMULI = [\n",
    "    \"\",\n",
    "    \"Write your answer and give me a confidence score between 0-1 for your answer.\",\n",
    "    \"This is very important to my career.\",\n",
    "    \"You'd better be sure.\",\n",
    "    \"Are you sure?\",\n",
    "    \"Are you sure that's your final answer? It might be worth taking another look.,\"\n",
    "    \"Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.\",\n",
    "    \"Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.\",\n",
    "    \"Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.\",\n",
    "    \"Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.\",\n",
    "    \"Take pride in your work and give it your best. Your commitment to excellence sets you apart.\",\n",
    "    \"Remember that progress is made one step at a time. Stay determined and keep moving forward.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0.5):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_score(score_data: ScoreData) -> float:\n",
    "    return 100 * (score_data.score_dict[score_data.preferred_score] - score_data.low_score) / (score_data.high_score - score_data.low_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_OF_DOMAIN = 1000\n",
    "\n",
    "class GPTModel(api_model.Model):\n",
    "    def __init__(self, model_name, *args, **kwargs):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate_text(self, inputs, max_length=500, stop_string=None, output_regex=None):\n",
    "        print(\"exact_str_match\")\n",
    "        if isinstance(inputs, str):\n",
    "            text = generate_chat_completion(inputs)\n",
    "        elif isinstance(inputs, list):\n",
    "            text = [\n",
    "                generate_chat_completion(input) for input in inputs\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"inputs has unexpected type %s\" % type(inputs))\n",
    "\n",
    "        # see README.md --> postprocess method --> \"exact_str_match\" tasks --> 1\n",
    "        if(\"Change tense to\" in inputs[0] or \"Change tense to\" in inputs[1]):\n",
    "            return text\n",
    "        if(\"Given a German language sentence\" in inputs[0]):\n",
    "            return text\n",
    "        \n",
    "        text = model_utils.postprocess_output(text, max_length, stop_string, output_regex)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def cond_log_prob(self, inputs, targets, absolute_normalization=False):\n",
    "        print(\"multiple_choice_grade\")\n",
    "        assert (not isinstance(targets, str)), \"targets in cond_log_prob must be a list (or a list of lists if inputs is a list). targets was instead a str.\"\n",
    "        \n",
    "\n",
    "        if isinstance(inputs, str):\n",
    "            text = generate_chat_completion(inputs)\n",
    "        elif isinstance(inputs, list):\n",
    "            text = [\n",
    "                generate_chat_completion(input) for input in inputs\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"inputs has unexpected type %s\" % type(inputs))\n",
    "        \n",
    "        probs = []\n",
    "        for i in range(len(inputs)):\n",
    "            choices = len(targets[i])\n",
    "            prob = [-np.inf] * choices\n",
    "            \n",
    "            chosen = False\n",
    "            for j in range(choices):\n",
    "                if targets[i][j].lower() == text[i].lower():\n",
    "                    prob[j] = 0\n",
    "                    chosen = True\n",
    "                    break\n",
    "            \n",
    "            if chosen: \n",
    "                probs.append(prob)\n",
    "                continue\n",
    "\n",
    "            # we need chaGPT4 to fix this part !!! (or other methods to fix it)\n",
    "            # Using chatGPT3.5 for budget concern first\n",
    "            check_ans_prompt = 'Please help me verify the answer return by another LLM model.'\n",
    "            check_ans_prompt += 'The answer list is ' + str(targets)\n",
    "            check_ans_prompt += 'The first anwser in the list is labeled as the number ' + str(0)\n",
    "            check_ans_prompt += 'The last anwser in the list is labeled as the number ' + str(choices-1)\n",
    "            check_ans_prompt += 'The answer return by the LLM model is ' + text[i]\n",
    "            check_ans_prompt += 'Please choose the number that best represent the answer return by the LLM model.'\n",
    "            check_ans_prompt += 'If the answer is not in the list, please choose the number ' + str(OUT_OF_DOMAIN)\n",
    "            check_ans_prompt += 'You are requested to give me an integer only!'\n",
    "            check_result = generate_chat_completion(check_ans_prompt)\n",
    "            print(check_result)\n",
    "            try:\n",
    "                # converting to integer\n",
    "                num = int(check_result)\n",
    "            except: TypeError\n",
    "            \n",
    "            if num >= 0 and num < choices:\n",
    "                prob[num] = 0\n",
    "            else:\n",
    "                prob[int(np.floor(np.random.rand()*choices))] = 0\n",
    "            probs.append(prob)\n",
    "\n",
    "        if len(inputs) == 1:\n",
    "            probs = probs[0]\n",
    "        return probs\n",
    "\n",
    "    def model_data(self, *args, **kwargs):\n",
    "        return api_model.ModelData(model_family='GPT-3.5', model_name='GPT-3.5-turbo',\n",
    "            total_params=175*(10^9), non_embedding_params=1,\n",
    "            flop_matched_non_embedding_params=1,\n",
    "            training_batch_size=1,\n",
    "            training_steps=1,\n",
    "            description='GPT-3.5-turbo',\n",
    "            decoding_params={}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in os.listdir(\"datasets\"):\n",
    "    if(dir == \".DS_Store\"):\n",
    "        continue\n",
    "    \n",
    "    # The output of this task is so fk strange ??! I think there's a bug!\n",
    "    # if(dir == \"causal_judgment\"):\n",
    "    #     continue\n",
    "\n",
    "    json_file_path = \"datasets/\" + dir + \"/task.json\"\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        \n",
    "        task = json.load(json_file)\n",
    "\n",
    "        # see README.md --> postprocess method --> \"multiple_choice_grade\" tasks\n",
    "        task['append_choices_to_input'] = True\n",
    "        if task[\"preferred_score\"] == 'multiple_choice_grade':\n",
    "            if \"task_prefix\" not in task.keys():\n",
    "                task[\"task_prefix\"] = \"Please only answer with the choice provided, any extra explanation is forbiddened.\"\n",
    "            else:\n",
    "                task[\"task_prefix\"] += \"Please only answer with the choice provided, any extra explanation is forbiddened.\"\n",
    "        \n",
    "        # see README.md --> postprocess method --> \"exact_str_match\" tasks --> 2\n",
    "        if task[\"name\"] == 'object_counting': \n",
    "            task[\"task_prefix\"] = \"Please answer numbers only.\"\n",
    "        \n",
    "        for emotional_stimulus in EMOTIONAL_STIMULI:\n",
    "            if \"task_prefix\" not in task.keys():\n",
    "                task[\"task_prefix\"] = emotional_stimulus\n",
    "            else:\n",
    "                task[\"task_prefix\"] += emotional_stimulus\n",
    "                \n",
    "            \n",
    "            current_task = json_task.JsonTask(\n",
    "                task_data=task,\n",
    "                shot_list=[0],\n",
    "                max_examples=5 # Should be 100 for final evaluation\n",
    "            )\n",
    "            model = GPTModel(\"gpt-3.5-turbo\")\n",
    "            score_data = current_task.evaluate_model(model)\n",
    "            print(score_data)\n",
    "            print(calculate_normalized_score(score_data[0]))\n",
    "            print(\"-------------------------------------------------\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
