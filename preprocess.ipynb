{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from bigbench.api import json_task\n",
    "import bigbench.api.model as api_model\n",
    "import bigbench.models.model_utils as model_utils\n",
    "from bigbench.api.task import ScoreData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONAL_STIMULI = [\n",
    "    \"\",\n",
    "    \"This is very important to my career.\",\n",
    "    \"This would appear in my interview tomorrow.\",\n",
    "    \"test breakpoint -------------------\",\n",
    "    \"Write your answer and give me a confidence score between 0-1 for your answer.\",\n",
    "    \"You'd better be sure.\",\n",
    "    \"Are you sure?\",\n",
    "    \"Are you sure that's your final answer? It might be worth taking another look.\",\n",
    "    \"Provide your answer and a confidence score between 0-1 for your prediction. Additionally, briefly explain the main reasons supporting your classification decision to help me understand your thought process. This task is vital to my career, and I greatly value your thorough analysis.\",\n",
    "    \"Are you sure that's your final answer? Believe in your abilities and strive for excellence. Your hard work will yield remarkable results.\",\n",
    "    \"Embrace challenges as opportunities for growth. Each obstacle you overcome brings you closer to success.\",\n",
    "    \"Stay focused and dedicated to your goals. Your consistent efforts will lead to outstanding achievements.\",\n",
    "    \"Take pride in your work and give it your best. Your commitment to excellence sets you apart.\",\n",
    "    \"Remember that progress is made one step at a time. Stay determined and keep moving forward.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0.5):\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_score(score_data: ScoreData) -> float:\n",
    "    return 100 * (score_data.score_dict[score_data.preferred_score] - score_data.low_score) / (score_data.high_score - score_data.low_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_OF_DOMAIN = 1000\n",
    "\n",
    "class GPTModel(api_model.Model):\n",
    "    def __init__(self, model_name, *args, **kwargs):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate_text(self, inputs, max_length=500, stop_string=None, output_regex=None):\n",
    "        # print(\"exact_str_match\")\n",
    "        if isinstance(inputs, str):\n",
    "            text = generate_chat_completion(inputs)\n",
    "        elif isinstance(inputs, list):\n",
    "            text = [\n",
    "                generate_chat_completion(input) for input in inputs\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"inputs has unexpected type %s\" % type(inputs))\n",
    "\n",
    "        # see README.md --> postprocess method --> \"exact_str_match\" tasks --> 1\n",
    "        if(\"Change tense to\" in inputs[0] or \"Change tense to\" in inputs[1]):\n",
    "            return text\n",
    "        if(\"Given a German language sentence\" in inputs[0]):\n",
    "            return text\n",
    "        \n",
    "        text = model_utils.postprocess_output(text, max_length, stop_string, output_regex)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def cond_log_prob(self, inputs, targets, absolute_normalization=False):\n",
    "        # print(\"multiple_choice_grade\")\n",
    "        assert (not isinstance(targets, str)), \"targets in cond_log_prob must be a list (or a list of lists if inputs is a list). targets was instead a str.\"\n",
    "        \n",
    "\n",
    "        if isinstance(inputs, str):\n",
    "            text = generate_chat_completion(inputs)\n",
    "        elif isinstance(inputs, list):\n",
    "            text = [\n",
    "                generate_chat_completion(input) for input in inputs\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"inputs has unexpected type %s\" % type(inputs))\n",
    "        \n",
    "        probs = []\n",
    "        for i in range(len(inputs)):\n",
    "            # print(targets[i])\n",
    "            # print(targets[i][:-1])\n",
    "            \n",
    "            choices = len(targets[i][:-1])\n",
    "            prob = [-np.inf] * choices\n",
    "            \n",
    "            chosen = False\n",
    "            for j in range(choices):\n",
    "                if targets[i][j].lower() == text[i].lower():\n",
    "                    prob[j] = 0\n",
    "                    chosen = True\n",
    "                    break\n",
    "            \n",
    "            if chosen: \n",
    "                probs.append(prob)\n",
    "                continue\n",
    "            # we need chaGPT4 to fix this part !!! (or other methods to fix it)\n",
    "            # Using chatGPT3.5 for budget concern first\n",
    "            check_ans_prompt = 'Please help me verify the answer return by another LLM model.\\n'\n",
    "            check_ans_prompt += 'The answer list is ' + str(targets[i][:-1]) + '.\\n'\n",
    "            check_ans_prompt += 'The first anwser in the list is labeled as the number ' + str(0) + '. '\n",
    "            check_ans_prompt += 'The last anwser in the list is labeled as the number ' + str(choices-1) + '.\\n'\n",
    "            check_ans_prompt += 'The answer return by the LLM model is \"' + text[i] + '\".\\n'\n",
    "            check_ans_prompt += 'Please choose the number that best represent the answer return by the LLM model. '\n",
    "            check_ans_prompt += 'If the answer is not in the list, please choose the number ' + str(OUT_OF_DOMAIN) + '.\\n'\n",
    "            check_ans_prompt += 'You are requested to give me an integer only!'\n",
    "            check_result = generate_chat_completion(check_ans_prompt)\n",
    "            try:\n",
    "                # converting to integer\n",
    "                num = int(check_result)\n",
    "                if num >= 0 and num < choices:\n",
    "                    prob[num] = 0\n",
    "                    probs.append(prob)\n",
    "                    continue\n",
    "\n",
    "            except: TypeError\n",
    "\n",
    "            print('------------------- Exception for all cases -------------------')\n",
    "            print(targets[i])\n",
    "            print(text[i])\n",
    "            print('------------------- End of Exception Report -------------------')\n",
    "            prob[-1] = 0\n",
    "            probs.append(prob)\n",
    "\n",
    "        if len(inputs) == 1:\n",
    "            probs = probs[0]\n",
    "            \n",
    "        return probs\n",
    "\n",
    "    def model_data(self, *args, **kwargs):\n",
    "        return api_model.ModelData(model_family='GPT-3.5', model_name='GPT-3.5-turbo',\n",
    "            total_params=175*(10^9), non_embedding_params=1,\n",
    "            flop_matched_non_embedding_params=1,\n",
    "            training_batch_size=1,\n",
    "            training_steps=1,\n",
    "            description='GPT-3.5-turbo',\n",
    "            decoding_params={}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for initializing score_for_diff_prompts\n",
    "score_for_diff_prompts = []\n",
    "for emotional_prompt_index in range(len(EMOTIONAL_STIMULI)):\n",
    "    emotional_stimulus = EMOTIONAL_STIMULI[emotional_prompt_index]\n",
    "    # for testing desired prompts only\n",
    "    if \"test breakpoint\" in emotional_stimulus: break\n",
    "    score_for_diff_prompts.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating epistemic_reasoning for 0 shots...\n",
      "evaluating epistemic_reasoning for 0 shots...\n",
      "evaluating epistemic_reasoning for 0 shots...\n",
      "evaluating navigate for 0 shots...\n",
      "evaluating navigate for 0 shots...\n",
      "evaluating navigate for 0 shots...\n",
      "evaluating gender_inclusive_sentences_german for 0 shots...\n",
      "evaluating gender_inclusive_sentences_german for 0 shots...\n",
      "evaluating gender_inclusive_sentences_german for 0 shots...\n",
      "evaluating causal_judgment for 0 shots...\n",
      "evaluating causal_judgment for 0 shots...\n",
      "evaluating causal_judgment for 0 shots...\n",
      "warning: operators has 1 duplicate examples out of 211\n",
      "evaluating operators for 0 shots...\n",
      "warning: operators has 1 duplicate examples out of 211\n",
      "evaluating operators for 0 shots...\n",
      "warning: operators has 1 duplicate examples out of 211\n",
      "evaluating operators for 0 shots...\n",
      "warning: question_selection has 8 duplicate examples out of 1590\n",
      "evaluating question_selection for 0 shots...\n",
      "------------------- Exception for all cases -------------------\n",
      "['When will the widening gap between the wealthiest people and the rest of the country begin to close?', \"When did economists come to an agreement with s&p's rating agency?\", 'I quit. This problem is nonsense.']\n",
      "When did the 2008-2009 recession occur?\n",
      "------------------- End of Exception Report -------------------\n",
      "warning: question_selection has 8 duplicate examples out of 1590\n",
      "evaluating question_selection for 0 shots...\n",
      "warning: question_selection has 8 duplicate examples out of 1590\n",
      "evaluating question_selection for 0 shots...\n",
      "warning: sports_understanding has 14 duplicate examples out of 1000\n",
      "evaluating sports_understanding for 0 shots...\n",
      "warning: sports_understanding has 14 duplicate examples out of 1000\n",
      "evaluating sports_understanding for 0 shots...\n",
      "warning: sports_understanding has 14 duplicate examples out of 1000\n",
      "evaluating sports_understanding for 0 shots...\n",
      "evaluating tense for 0 shots...\n",
      "evaluating tense for 0 shots...\n",
      "evaluating tense for 0 shots...\n",
      "warning: winowhy has 3 duplicate examples out of 2865\n",
      "evaluating winowhy for 0 shots...\n",
      "warning: winowhy has 3 duplicate examples out of 2865\n",
      "evaluating winowhy for 0 shots...\n",
      "warning: winowhy has 3 duplicate examples out of 2865\n",
      "evaluating winowhy for 0 shots...\n",
      "evaluating dyck_languages for 0 shots...\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "< < ( ) )\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "[ { { { ( ) } { < [ < < > > ] > } } ] ]\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "[ { < [ [ < [ < < > > ] > ( ) < { ( ( ( ( { } ) ) ) ) } > ] ] { } ( ) > } ] [ ] { { ( { } } ] ]\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "( [ ( ( [ [ < ( ) > ] ] ) ] )\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "( < > ( [ < [ ( ( [ [ ] ] < { ( { < > } [ { < ( ) > [ < < > > ] < { [ < [ ( ( < ( [ < < > > ] ) > ) ) ] > ] } { [ [ ( ) { < { } > } ] ] } [ ] > } ] ) } > ) ) ] > ] }\n",
      "------------------- End of Exception Report -------------------\n",
      "evaluating dyck_languages for 0 shots...\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "( ) >\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "] ] ] >\n",
      "------------------- End of Exception Report -------------------\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "} ] ])\n",
      "------------------- End of Exception Report -------------------\n",
      "evaluating dyck_languages for 0 shots...\n",
      "------------------- Exception for all cases -------------------\n",
      "[')', ']', '>', '}', ') )', ') ]', ') >', ') }', '] )', '] ]', '] >', '] }', '> )', '> ]', '> >', '> }', '} )', '} ]', '} >', '} }', ') ) )', ') ) ]', ') ) >', ') ) }', ') ] )', ') ] ]', ') ] >', ') ] }', ') > )', ') > ]', ') > >', ') > }', ') } )', ') } ]', ') } >', ') } }', '] ) )', '] ) ]', '] ) >', '] ) }', '] ] )', '] ] ]', '] ] >', '] ] }', '] > )', '] > ]', '] > >', '] > }', '] } )', '] } ]', '] } >', '] } }', '> ) )', '> ) ]', '> ) >', '> ) }', '> ] )', '> ] ]', '> ] >', '> ] }', '> > )', '> > ]', '> > >', '> > }', '> } )', '> } ]', '> } >', '> } }', '} ) )', '} ) ]', '} ) >', '} ) }', '} ] )', '} ] ]', '} ] >', '} ] }', '} > )', '} > ]', '} > >', '} > }', '} } )', '} } ]', '} } >', '} } }', 'I quit. This problem is nonsense.']\n",
      "[ { < [ [ < [ < < > > ] > ( ) < { ( ( ( ( { } ) ) ) ) } > ] ] { } ( ) > } ] [ ] { { ( { } } ] )\n",
      "------------------- End of Exception Report -------------------\n",
      "evaluating implicatures for 0 shots...\n",
      "evaluating implicatures for 0 shots...\n",
      "evaluating implicatures for 0 shots...\n",
      "evaluating linguistics_puzzles for 0 shots...\n",
      "evaluating linguistics_puzzles for 0 shots...\n",
      "evaluating linguistics_puzzles for 0 shots...\n",
      "evaluating disambiguation_qa for 0 shots...\n",
      "evaluating disambiguation_qa for 0 shots...\n",
      "evaluating disambiguation_qa for 0 shots...\n",
      "evaluating logical_fallacy_detection for 0 shots...\n",
      "evaluating logical_fallacy_detection for 0 shots...\n",
      "evaluating logical_fallacy_detection for 0 shots...\n",
      "evaluating movie_recommendation for 0 shots...\n",
      "evaluating movie_recommendation for 0 shots...\n",
      "evaluating movie_recommendation for 0 shots...\n",
      "evaluating snarks for 0 shots...\n",
      "evaluating snarks for 0 shots...\n",
      "evaluating snarks for 0 shots...\n",
      "evaluating presuppositions_as_nli for 0 shots...\n",
      "evaluating presuppositions_as_nli for 0 shots...\n",
      "evaluating presuppositions_as_nli for 0 shots...\n",
      "evaluating word_unscrambling for 0 shots...\n",
      "evaluating word_unscrambling for 0 shots...\n",
      "evaluating word_unscrambling for 0 shots...\n",
      "evaluating word_sorting for 0 shots...\n",
      "evaluating word_sorting for 0 shots...\n",
      "evaluating word_sorting for 0 shots...\n",
      "evaluating ruin_names for 0 shots...\n",
      "evaluating ruin_names for 0 shots...\n",
      "evaluating ruin_names for 0 shots...\n",
      "evaluating object_counting for 0 shots...\n",
      "evaluating object_counting for 0 shots...\n",
      "evaluating object_counting for 0 shots...\n",
      "[812.2952068354423, 824.7848789242613, 833.3620510362801]\n",
      "Total 21 tasks are done for each prompt.\n",
      "[38.68072413502106, 39.27547042496482, 39.68390719220382]\n"
     ]
    }
   ],
   "source": [
    "for dir in os.listdir(\"datasets\"):\n",
    "    if(dir == \".DS_Store\"):\n",
    "        continue\n",
    "\n",
    "    json_file_path = \"datasets/\" + dir + \"/task.json\"\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        \n",
    "        task = json.load(json_file)\n",
    "\n",
    "        # adding \"choice: Ambiguous answer which sho\" with 0 score to all multiple_choice_grade tasks\n",
    "        if task[\"preferred_score\"] == 'multiple_choice_grade':\n",
    "            for example in task[\"examples\"]:\n",
    "                example[\"target_scores\"][\"I quit. This problem is nonsense.\"] = 0\n",
    "\n",
    "        # see README.md --> postprocess method --> \"multiple_choice_grade\" tasks\n",
    "        if task[\"preferred_score\"] == 'multiple_choice_grade':\n",
    "            task['append_choices_to_input'] = True\n",
    "\n",
    "            if \"task_prefix\" not in task.keys():\n",
    "                task[\"task_prefix\"] = \"Please only answer with the choice provided, any extra explanation is forbiddened. You are also not allowed to choose the last choice, which is quiting!\"\n",
    "            else:\n",
    "                task[\"task_prefix\"] += \" Please only answer with the choice provided, any extra explanation is forbiddened. You are also not allowed to choose the last choice, which is quiting!\"\n",
    "            \n",
    "        \n",
    "        # see README.md --> postprocess method --> \"exact_str_match\" tasks --> 2\n",
    "        if task[\"name\"] == 'object_counting': \n",
    "            task[\"task_prefix\"] = \"Please answer numbers only.\"\n",
    "        \n",
    "        if \"task_prefix\" not in task.keys():\n",
    "            original_task_prefix = ''\n",
    "        else:\n",
    "            original_task_prefix = task[\"task_prefix\"] + ' '\n",
    "\n",
    "        for emotional_prompt_index in range(len(EMOTIONAL_STIMULI)):\n",
    "            emotional_stimulus = EMOTIONAL_STIMULI[emotional_prompt_index]\n",
    "            # for testing desired prompts only\n",
    "            if \"test breakpoint\" in emotional_stimulus: break\n",
    "\n",
    "            task[\"task_prefix\"] = original_task_prefix + emotional_stimulus + ' '\n",
    "            \n",
    "            current_task = json_task.JsonTask(\n",
    "                task_data=task,\n",
    "                shot_list=[0],\n",
    "                max_examples=10 # Should be 100 for final evaluation\n",
    "            )\n",
    "\n",
    "            model = GPTModel(\"gpt-3.5-turbo\")\n",
    "            score_data = current_task.evaluate_model(model)\n",
    "            # print(score_data)\n",
    "            # print(calculate_normalized_score(score_data[0]))\n",
    "            score_for_diff_prompts[emotional_prompt_index] += (calculate_normalized_score(score_data[0]))\n",
    "            # print(\"-------------------------------------------------\")\n",
    "\n",
    "print(score_for_diff_prompts)\n",
    "print('Total ' + str(21) + ' tasks are done for each prompt.')\n",
    "score_for_diff_prompts = [(x / 21) for x in score_for_diff_prompts]\n",
    "# final result\n",
    "print(score_for_diff_prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_prompt_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
